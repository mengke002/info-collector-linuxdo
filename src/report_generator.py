"""
æ™ºèƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
åŸºäºæ¿å—çš„çƒ­ç‚¹å†…å®¹åˆ†æå’ŒMarkdownæŠ¥å‘Šç”Ÿæˆ
"""
import logging
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone, timedelta

from .database import db_manager
from .llm_client import llm_client
from .config import config


class ReportGenerator:
    """æ™ºèƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db = db_manager
        self.llm = llm_client
        
        # æŠ¥å‘Šé…ç½®
        report_config = config.get_report_config()
        self.top_topics_per_category = report_config.get('top_topics_per_category', 35)
        self.top_replies_per_topic = 10
        # å¢åŠ å†…å®¹é•¿åº¦é™åˆ¶ä»¥å®¹çº³æ›´å¤šä¸»é¢˜
        self.max_content_length = config.get_llm_config().get('max_content_length', 50000)
    
    def get_beijing_time(self) -> datetime:
        """è·å–å½“å‰åŒ—äº¬æ—¶é—´"""
        return datetime.now(timezone.utc) + timedelta(hours=8)
    
    def _truncate_content(self, content: str, max_length: int = None) -> str:
        """æˆªæ–­å†…å®¹åˆ°æŒ‡å®šé•¿åº¦"""
        if max_length is None:
            max_length = self.max_content_length
        
        if len(content) <= max_length:
            return content
        
        # åœ¨åˆé€‚çš„ä½ç½®æˆªæ–­ï¼Œé¿å…æˆªæ–­åˆ°å¥å­ä¸­é—´
        truncated = content[:max_length]
        
        # å°è¯•åœ¨æœ€åä¸€ä¸ªå¥å·ã€é—®å·æˆ–æ„Ÿå¹å·å¤„æˆªæ–­
        for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
            last_delimiter = truncated.rfind(delimiter)
            if last_delimiter > max_length * 0.8:  # ç¡®ä¿ä¸ä¼šæˆªæ‰å¤ªå¤šå†…å®¹
                return truncated[:last_delimiter + 1]
        
        # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œå°±åœ¨æœ€åä¸€ä¸ªç©ºæ ¼å¤„æˆªæ–­
        last_space = truncated.rfind(' ')
        if last_space > max_length * 0.8:
            return truncated[:last_space] + "..."
        
        return truncated + "..."
    

    
    def _generate_report_markdown(self, category: str, analysis_results: List[Dict[str, Any]], 
                                 period_start: datetime, period_end: datetime) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        
        # æŠ¥å‘Šæ ‡é¢˜
        title = f"ğŸ“ˆ [{category}] æ¿å—24å°æ—¶çƒ­ç‚¹æŠ¥å‘Š"
        
        # æ—¶é—´ä¿¡æ¯
        start_str = period_start.strftime('%Y-%m-%d %H:%M:%S')
        end_str = period_end.strftime('%Y-%m-%d %H:%M:%S')
        generate_time = self.get_beijing_time().strftime('%Y-%m-%d %H:%M:%S')
        
        # æ„å»ºæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"# {title}",
            "",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {generate_time}*",
            "",
            f"*æ•°æ®èŒƒå›´: {start_str} - {end_str}*",
            "",
            "---",
            "",
            f"## ğŸ”¥ æœ¬æ—¶æ®µçƒ­é—¨ä¸»é¢˜ Top {len(analysis_results)}",
            ""
        ]
        
        # æ·»åŠ æ¯ä¸ªä¸»é¢˜çš„åˆ†æ
        for i, result in enumerate(analysis_results, 1):
            report_lines.extend([
                f"### {i}. {result['title']}",
                f"- **åŸå§‹é“¾æ¥**: [{result['url']}]({result['url']})",
                f"- **çƒ­åº¦åˆ†æ•°**: {result['hotness_score']:.2f}",
                ""
            ])
            
            # è§£æLLMåˆ†æç»“æœ
            analysis_content = result.get('analysis', '')
            if analysis_content:
                # ç®€å•è§£æåˆ†æç»“æœï¼Œæå–æ‘˜è¦å’Œå…³é”®ç‚¹
                lines = analysis_content.split('\n')
                current_section = None
                
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    if line.startswith('## æ ¸å¿ƒæ‘˜è¦') or line.startswith('## æ‘˜è¦'):
                        current_section = 'summary'
                        continue
                    elif line.startswith('## å…³é”®ä¿¡æ¯ç‚¹') or line.startswith('## å…³é”®ç‚¹'):
                        current_section = 'points'
                        report_lines.append("- **å…³é”®è®¨è®ºç‚¹**:")
                        continue
                    elif line.startswith('##'):
                        current_section = None
                        continue
                    
                    if current_section == 'summary' and not line.startswith('-'):
                        # æ‘˜è¦å†…å®¹
                        if line:
                            report_lines.append(f"  > {line}")
                    elif current_section == 'points' and line.startswith('-'):
                        # å…³é”®ä¿¡æ¯ç‚¹
                        point = line[1:].strip()
                        if point:
                            report_lines.append(f"  {line}")
                
                # å¦‚æœæ²¡æœ‰æˆåŠŸè§£æï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹åˆ†æç»“æœ
                if current_section is None and analysis_content:
                    report_lines.extend([
                        "- **åˆ†æç»“æœ**:",
                        f"  > {analysis_content.replace(chr(10), chr(10) + '  > ')}"
                    ])
            
            # æŠ€æœ¯ä¿¡æ¯ï¼ˆå¯é€‰æ˜¾ç¤ºï¼‰
            if result.get('provider'):
                report_lines.append(f"- *åˆ†æå¼•æ“: {result['provider']} ({result.get('model', 'unknown')})*")
            
            report_lines.extend(["", "---", ""])
        
        # æŠ¥å‘Šå°¾éƒ¨
        report_lines.extend([
            "",
            f"ğŸ“Š **ç»Ÿè®¡æ‘˜è¦**: æœ¬æŠ¥å‘Šåˆ†æäº† {len(analysis_results)} ä¸ªçƒ­é—¨ä¸»é¢˜",
            "",
            "*æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*"
        ])
        
        return "\n".join(report_lines)

    def _enhance_source_links(self, report_content: str, hot_topics_data: List[Dict[str, Any]]) -> str:
        """
        å¢å¼ºæŠ¥å‘Šä¸­çš„æ¥æºé“¾æ¥ï¼Œå°† [Source: T1, T2] ä¸­çš„æ¯ä¸ª Txx è½¬æ¢ä¸ºå¯ç‚¹å‡»çš„é“¾æ¥
        """
        import re

        # æ„å»ºæ¥æºIDåˆ°é“¾æ¥çš„æ˜ å°„
        source_link_map = {}
        for i, topic_data in enumerate(hot_topics_data, 1):
            topic_info = topic_data['topic']
            source_link_map[f'T{i}'] = topic_info.get('url', '')

        def replace_source_refs(match):
            # æå–å®Œæ•´çš„ Source å¼•ç”¨å†…å®¹
            full_source_text = match.group(0)  # å¦‚ "[Source: T2, T9, T18]"
            source_content = match.group(1)    # å¦‚ "T2, T9, T18"

            # åˆ†å‰²å¹¶å¤„ç†æ¯ä¸ªæ¥æºID
            source_ids = [sid.strip() for sid in source_content.split(',')]
            linked_sources = []

            for sid in source_ids:
                if sid in source_link_map:
                    # å°† Txx è½¬æ¢ä¸ºé“¾æ¥
                    linked_sources.append(f"[{sid}]({source_link_map[sid]})")
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”é“¾æ¥ï¼Œä¿æŒåŸæ ·
                    linked_sources.append(sid)

            # é‡æ–°ç»„åˆ
            return f"ğŸ“ [Source: {', '.join(linked_sources)}]"

        # æŸ¥æ‰¾æ‰€æœ‰ [Source: ...] æˆ– [Sources: ...] æ¨¡å¼å¹¶æ›¿æ¢
        pattern = r'\[Sources?:\s*([T\d\s,]+)\]'
        enhanced_content = re.sub(pattern, replace_source_refs, report_content)

        return enhanced_content

    def _generate_unified_report_markdown(self, category: str, unified_analysis: Dict[str, Any],
                                         hot_topics_data: List[Dict[str, Any]],
                                         period_start: datetime, period_end: datetime) -> str:
        """ç”Ÿæˆç»Ÿä¸€åˆ†ææŠ¥å‘Šçš„Markdownæ ¼å¼ (V2.1)"""

        # æŠ¥å‘Šæ ‡é¢˜
        title = f"ğŸ“ˆ [{category}] ç¤¾åŒºæƒ…æŠ¥æ´å¯ŸæŠ¥å‘Š"

        # æ—¶é—´ä¿¡æ¯
        start_str = period_start.strftime('%Y-%m-%d %H:%M:%S')
        end_str = period_end.strftime('%Y-%m-%d %H:%M:%S')
        generate_time = self.get_beijing_time().strftime('%Y-%m-%d %H:%M:%S')

        # è·å–AIåˆ†æå†…å®¹
        analysis_content = unified_analysis.get('analysis', 'åˆ†æå†…å®¹ç”Ÿæˆå¤±è´¥ã€‚')

        # æ„å»ºæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"# {title}",
            "",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {generate_time}*",
            "",
            f"*æ•°æ®èŒƒå›´: {start_str} - {end_str}*",
            "",
            "---",
            "",
            analysis_content,  # æ’å…¥LLMç”Ÿæˆçš„å®Œæ•´æŠ¥å‘Š
            "",
            "---",
            "",
            "## ğŸ“š æ¥æºæ¸…å• (Source List)",
            ""
        ]

        # ç”Ÿæˆæ¥æºæ¸…å•
        for i, topic_data in enumerate(hot_topics_data, 1):
            topic_info = topic_data['topic']
            # å°†æ ‡é¢˜ä¸­çš„æ–¹æ‹¬å·æ›¿æ¢ä¸ºä¸­æ–‡æ–¹æ‹¬å·ï¼Œé¿å…å¹²æ‰°Markdowné“¾æ¥è§£æ
            clean_title = topic_info['title'].replace('[', 'ã€').replace(']', 'ã€‘')
            # æ·»åŠ é”šç‚¹æ ‡è¯†ï¼Œæ–¹ä¾¿å†…éƒ¨å¼•ç”¨
            report_lines.append(
                f"- **[T{i}]** ğŸ“Œ: [{clean_title}]({topic_info['url']})"
            )

        report_lines.extend(["", "---", ""])

        # æŠ€æœ¯ä¿¡æ¯
        if unified_analysis.get('provider'):
            report_lines.append(
                f"*åˆ†æå¼•æ“: {unified_analysis['provider']} ({unified_analysis.get('model', 'unknown')})*"
            )

        report_lines.extend([
            "",
            f"ğŸ“Š **ç»Ÿè®¡æ‘˜è¦**: æœ¬æŠ¥å‘Šåˆ†æäº† {len(hot_topics_data)} ä¸ªçƒ­é—¨ä¸»é¢˜",
            "",
            "*æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*"
        ])

        # ç”ŸæˆåŸå§‹æŠ¥å‘Šå†…å®¹
        raw_report = "\n".join(report_lines)

        # å¢å¼ºæºé“¾æ¥ï¼Œå°† [Source: T1, T2] è½¬æ¢ä¸ºå¯ç‚¹å‡»é“¾æ¥
        enhanced_report = self._enhance_source_links(raw_report, hot_topics_data)

        return enhanced_report

    async def generate_category_report(self, category: str = None, hours_back: int = 24) -> Dict[str, Any]:
        """ç”Ÿæˆçƒ­ç‚¹åˆ†ææŠ¥å‘Šï¼ˆä¸å†æŒ‰åˆ†ç±»ç­›é€‰ï¼Œä»æ‰€æœ‰æ•°æ®ä¸­è·å–çƒ­é—¨ä¸»é¢˜ï¼‰"""
        try:
            report_title = "å…¨ç«™çƒ­ç‚¹åˆ†ææŠ¥å‘Š" if category is None else f"{category} æ¿å—çƒ­ç‚¹åˆ†ææŠ¥å‘Š"
            self.logger.info(f"å¼€å§‹ç”Ÿæˆ {report_title} (å›æº¯ {hours_back} å°æ—¶)")
            
            # è®¾ç½®åˆ†ææ—¶é—´èŒƒå›´
            end_time = self.get_beijing_time()
            start_time = end_time - timedelta(hours=hours_back)
            
            # è·å–æ‰€æœ‰çƒ­é—¨ä¸»é¢˜ï¼ˆä¸æŒ‰åˆ†ç±»ç­›é€‰ï¼‰
            hot_topics = self.db.get_hot_topics_all(
                limit=self.top_topics_per_category, 
                hours_back=hours_back
            )
            
            if not hot_topics:
                self.logger.warning(f"è¿‡å» {hours_back} å°æ—¶å†…æ²¡æœ‰çƒ­é—¨ä¸»é¢˜")
                return {
                    'success': True,
                    'category': category or 'å…¨ç«™',
                    'topics_analyzed': 0,
                    'message': f'è¿‡å» {hours_back} å°æ—¶å†…æš‚æ— çƒ­é—¨å†…å®¹'
                }
            
            self.logger.info(f"æ‰¾åˆ° {len(hot_topics)} ä¸ªçƒ­é—¨ä¸»é¢˜ï¼Œå¼€å§‹è·å–è¯¦ç»†æ•°æ®")
            
            # è·å–æ‰€æœ‰ä¸»é¢˜çš„è¯¦ç»†æ•°æ®
            hot_topics_data = []
            for i, topic in enumerate(hot_topics, 1):
                self.logger.info(f"è·å–ç¬¬ {i}/{len(hot_topics)} ä¸ªä¸»é¢˜è¯¦ç»†æ•°æ®: {topic.get('title', 'æœªçŸ¥æ ‡é¢˜')[:50]}...")
                
                # è·å–ä¸»é¢˜çš„è¯¦ç»†å†…å®¹
                topic_data = self.db.get_topic_posts_for_analysis(
                    topic['id'], 
                    limit=self.top_replies_per_topic
                )
                
                if topic_data:
                    hot_topics_data.append(topic_data)
                    self.logger.info(f"ä¸»é¢˜ {i}/{len(hot_topics)} æ•°æ®è·å–æˆåŠŸ")
                else:
                    self.logger.warning(f"ä¸»é¢˜ {i}/{len(hot_topics)} æ— æ³•è·å–è¯¦ç»†æ•°æ®")
            
            if not hot_topics_data:
                self.logger.warning(f"æ‰€æœ‰ä¸»é¢˜éƒ½æ— æ³•è·å–è¯¦ç»†æ•°æ®")
                return {
                    'success': False,
                    'error': 'æ— æ³•è·å–ä¸»é¢˜è¯¦ç»†æ•°æ®',
                    'category': category,
                    'topics_analyzed': 0
                }
            
            self.logger.info(f"å…±è·å–åˆ° {len(hot_topics_data)} ä¸ªä¸»é¢˜çš„è¯¦ç»†æ•°æ®ï¼Œå¼€å§‹ç»Ÿä¸€LLMåˆ†æ")
            
            # ä½¿ç”¨ç»Ÿä¸€LLMåˆ†æ
            unified_result = self._analyze_all_topics_with_llm(hot_topics_data)
            
            if not unified_result or not unified_result.get('success'):
                self.logger.warning(f"{category or 'å…¨ç«™'} æ¿å—çš„ç»Ÿä¸€ä¸»é¢˜åˆ†æå¤±è´¥")
                return {
                    'success': False,
                    'error': f'{category or "å…¨ç«™"} æ¿å—ä¸»é¢˜åˆ†æå¤±è´¥',
                    'category': category,
                    'topics_analyzed': 0
                }
            
            self.logger.info(f"ç»Ÿä¸€åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(hot_topics_data)} ä¸ªä¸»é¢˜")
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            report_content = self._generate_unified_report_markdown(
                category=category or 'å…¨ç«™',
                unified_analysis=unified_result,
                hot_topics_data=hot_topics_data,
                period_start=start_time,
                period_end=end_time
            )
            
            # ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“
            report_data = {
                'category': category or 'å…¨ç«™',
                'report_type': 'hotspot',
                'analysis_period_start': start_time,
                'analysis_period_end': end_time,
                'topics_analyzed': len(hot_topics_data),
                'report_title': f'[{category or "å…¨ç«™"}] ç¤¾åŒºæƒ…æŠ¥æ´å¯ŸæŠ¥å‘Š',
                'report_content': report_content
            }
            
            report_id = self.db.save_report(report_data)
            
            result = {
                'success': True,
                'category': category or 'å…¨ç«™',
                'report_id': report_id,
                'topics_analyzed': len(hot_topics_data),
                'total_topics_found': len(hot_topics),
                'analysis_period': {
                    'start': start_time,
                    'end': end_time,
                    'hours_back': hours_back
                },
                'report_preview': report_content[:500] + "..." if len(report_content) > 500 else report_content
            }
            
            # å°è¯•æ¨é€åˆ°Notion
            try:
                from .notion_client import notion_client
                
                # æ ¼å¼åŒ–Notionæ ‡é¢˜
                beijing_time = self.get_beijing_time()
                time_str = beijing_time.strftime('%H:%M')
                notion_title = f"[{time_str}] {category or 'å…¨ç«™'}çƒ­ç‚¹æŠ¥å‘Š ({len(hot_topics_data)}ä¸ªä¸»é¢˜)"
                
                self.logger.info(f"å¼€å§‹æ¨é€æŠ¥å‘Šåˆ°Notion: {notion_title}")
                
                notion_result = notion_client.create_report_page(
                    report_title=notion_title,
                    report_content=report_content,
                    report_date=beijing_time
                )
                
                if notion_result.get('success'):
                    self.logger.info(f"æŠ¥å‘ŠæˆåŠŸæ¨é€åˆ°Notion: {notion_result.get('page_url')}")
                    result['notion_push'] = {
                        'success': True,
                        'page_url': notion_result.get('page_url'),
                        'path': notion_result.get('path')
                    }
                else:
                    self.logger.warning(f"æ¨é€åˆ°Notionå¤±è´¥: {notion_result.get('error')}")
                    result['notion_push'] = {
                        'success': False,
                        'error': notion_result.get('error')
                    }
                    
            except Exception as e:
                self.logger.warning(f"æ¨é€åˆ°Notionæ—¶å‡ºé”™: {e}")
                result['notion_push'] = {
                    'success': False,
                    'error': str(e)
                }
            
            self.logger.info(f"{category or 'å…¨ç«™'} åˆ†æå®Œæˆ: åˆ†æäº† {len(hot_topics_data)}/{len(hot_topics)} ä¸ªä¸»é¢˜ï¼ŒæŠ¥å‘ŠID: {report_id}")
            return result
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ {category or 'å…¨ç«™'} æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            return {
                'success': False,
                'error': str(e),
                'category': category or 'å…¨ç«™',
                'topics_analyzed': 0
            }
    
    async def generate_all_categories_report(self, hours_back: int = 24) -> Dict[str, Any]:
        """ç”Ÿæˆå…¨ç«™çƒ­ç‚¹åˆ†ææŠ¥å‘Šï¼ˆä¸å†æŒ‰åˆ†ç±»ï¼‰"""
        try:
            self.logger.info(f"å¼€å§‹ç”Ÿæˆå…¨ç«™çƒ­ç‚¹åˆ†ææŠ¥å‘Š (å›æº¯ {hours_back} å°æ—¶)")
            
            # ç›´æ¥ç”Ÿæˆä¸€ä¸ªå…¨ç«™æŠ¥å‘Š
            result = await self.generate_category_report(category=None, hours_back=hours_back)
            
            if result.get('success'):
                # åŒ…è£…æˆä¸åŸæ ¼å¼å…¼å®¹çš„ç»“æ„
                return {
                    'success': True,
                    'total_categories': 1,
                    'successful_reports': 1,
                    'failed_reports': 0,
                    'total_topics_analyzed': result.get('topics_analyzed', 0),
                    'reports': [result],
                    'failures': [],
                    'generation_time': self.get_beijing_time()
                }
            else:
                return {
                    'success': False,
                    'total_categories': 1,
                    'successful_reports': 0,
                    'failed_reports': 1,
                    'total_topics_analyzed': 0,
                    'reports': [],
                    'failures': [{
                        'category': 'å…¨ç«™',
                        'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                    }],
                    'generation_time': self.get_beijing_time()
                }
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå…¨ç«™æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            return {
                'success': False,
                'error': str(e),
                'reports': []
            }
    
    def _format_all_topics_for_analysis(self, hot_topics_data: List[Dict[str, Any]]) -> str:
        """å°†æ‰€æœ‰çƒ­é—¨ä¸»é¢˜åˆå¹¶ä¸ºä¸€ä¸ªæ–‡æ¡£ç”¨äºLLMç»Ÿä¸€åˆ†æ (V2.1)"""
        content_parts = []
        
        # æ·»åŠ æ–‡æ¡£å¤´éƒ¨
        content_parts.extend([
            "=== çƒ­é—¨ä¸»é¢˜ç»¼åˆåˆ†ææ–‡æ¡£ ===",
            f"æ€»è®¡ {len(hot_topics_data)} ä¸ªçƒ­é—¨ä¸»é¢˜",
            "",
        ])
        
        # æŒ‰çƒ­åº¦æ’åºå¤„ç†æ¯ä¸ªä¸»é¢˜
        for i, topic_data in enumerate(hot_topics_data, 1):
            topic_info = topic_data['topic']
            main_post = topic_data.get('main_post')
            replies = topic_data.get('replies', [])
            
            content_parts.extend([
                f"\n### [Source: T{i}] {topic_info['title']}",
                f"çƒ­åº¦åˆ†æ•°: {topic_info.get('hotness_score', 0):.2f}",
                f"åˆ†ç±»: {topic_info.get('category', 'æœªçŸ¥')}",
                f"å›å¤æ•°: {topic_info.get('reply_count', 0)}",
                f"æµè§ˆæ•°: {topic_info.get('view_count', 0)}",
                f"æ€»ç‚¹èµæ•°: {topic_info.get('total_like_count', 0)}",
                f"URL: {topic_info.get('url', '')}",
                ""
            ])
            
            # ä¸»è´´å†…å®¹ï¼ˆç²¾ç®€ç‰ˆï¼‰
            if main_post and main_post.get('content_raw'):
                main_content = main_post['content_raw'].strip()
                if main_content:
                    # é™åˆ¶ä¸»è´´å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                    if len(main_content) > 800:
                        main_content = main_content[:800] + "..."
                    content_parts.extend([
                        "**ä¸»è´´å†…å®¹:**",
                        main_content,
                        ""
                    ])
            
            # çƒ­é—¨å›å¤ï¼ˆç²¾ç®€ç‰ˆï¼‰
            if replies:
                content_parts.append("**çƒ­é—¨å›å¤:**")
                # é™åˆ¶å›å¤æ•°é‡å’Œé•¿åº¦
                for j, reply in enumerate(replies[:min(3, self.top_replies_per_topic)], 1):
                    if reply.get('content_raw'):
                        reply_content = reply['content_raw'].strip()
                        if reply_content:
                            # é™åˆ¶å›å¤å†…å®¹é•¿åº¦
                            if len(reply_content) > 200:
                                reply_content = reply_content[:200] + "..."
                            content_parts.extend([
                                f"{j}. (ç‚¹èµ: {reply.get('like_count', 0)}): {reply_content}",
                                ""
                            ])
            
            content_parts.append("---\n")  # ä¸»é¢˜åˆ†å‰²çº¿
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        full_content = "\n".join(content_parts)
        
        # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œè¿™é‡Œä¸å†æˆªæ–­ï¼Œè®©LLMçœ‹åˆ°æ‰€æœ‰ä¸»é¢˜
        self.logger.info(f"æ ¼å¼åŒ–åçš„ä¸»é¢˜å†…å®¹æ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")
        return full_content
    
    def _get_unified_analysis_prompt_template(self) -> str:
        """è·å–ç»Ÿä¸€åˆ†æçš„æç¤ºè¯æ¨¡æ¿ï¼ˆV2.4 - Linuxdo æ·±åº¦å†…å®¹ç‰ˆï¼‰"""
        return """ä½ æ˜¯ä¸€ä½ç†Ÿæ‚‰å¼€å‘è€…æ–‡åŒ–å’ŒæŠ€æœ¯ç”Ÿæ€çš„èµ„æ·±ç¤¾åŒºåˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä»¥ä¸‹æ¥è‡ª Linuxdo ç¤¾åŒºçš„ã€å·²ç¼–å·çš„åŸå§‹è®¨è®ºææ–™ï¼Œå¹¶ä¸ºå¹¿å¤§çš„å¼€å‘è€…å’ŒAIçˆ±å¥½è€…æ’°å†™ä¸€ä»½ä¿¡æ¯å¯†åº¦é«˜ã€å†…å®¹è¯¦å°½ã€å¯è¯»æ€§å¼ºçš„æƒ…æŠ¥ç®€æŠ¥ã€‚

**åˆ†æåŸåˆ™:**
1.  **ä»·å€¼å¯¼å‘ä¸æ·±åº¦ä¼˜å…ˆ**: ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯æŒ–æ˜å‡ºå¯¹å¼€å‘è€…å’ŒAIçˆ±å¥½è€…æœ‰ç›´æ¥ä»·å€¼çš„ä¿¡æ¯ã€‚åœ¨æ’°å†™æ¯ä¸ªéƒ¨åˆ†æ—¶ï¼Œéƒ½åº”è¿½æ±‚å†…å®¹çš„æ·±åº¦å’Œå®Œæ•´æ€§ï¼Œé¿å…è¿‡äºç®€çŸ­çš„æ¦‚æ‹¬ã€‚
2.  **å¿ äºåŸæ–‡ä¸å¯è¿½æº¯æ€§**: æ‰€æœ‰åˆ†æéƒ½å¿…é¡»åŸºäºåŸæ–‡ï¼Œå¹¶ä¸”æ¯ä¸€æ¡ç»“è®ºéƒ½å¿…é¡»åœ¨å¥æœ«ä½¿ç”¨ `[Source: T_n]` æˆ– `[Sources: T_n, T_m]` çš„æ ¼å¼æ˜ç¡®æ ‡æ³¨æ¥æºã€‚
3.  **è¯†åˆ«å¸–å­ç±»å‹**: åœ¨åˆ†ææ—¶ï¼Œè¯·æ³¨æ„è¯†åˆ«æ¯ä¸ªä¸»é¢˜çš„æ½œåœ¨ç±»å‹ï¼Œä¾‹å¦‚ï¼šæŠ€æœ¯åˆ†äº«ã€é—®é¢˜æ±‚åŠ©ã€èµ„æºå‘å¸ƒã€ä¿¡æ¯å‘ç°ã€è§‚ç‚¹è®¨è®ºã€ç¤¾åŒºæ´»åŠ¨ç­‰ã€‚è¿™æœ‰åŠ©äºä½ åˆ¤æ–­å…¶æ ¸å¿ƒä»·å€¼ã€‚

---

**åŸå§‹è®¨è®ºææ–™ (å·²ç¼–å·):**
{content}

---

**ä½ çš„æŠ¥å‘Šç”Ÿæˆä»»åŠ¡:**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„å’Œè¦æ±‚ï¼Œç”Ÿæˆä¸€ä»½å†…å®¹ä¸°å¯Œè¯¦å®çš„å®Œæ•´MarkdownæŠ¥å‘Šã€‚

**ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ¬æ—¶æ®µç„¦ç‚¹é€ŸæŠ¥ (Top Topics Overview)**
*   ä»»åŠ¡ï¼šé€šè¯»æ‰€æœ‰ææ–™ï¼Œä¸ºæ¯ä¸ªå€¼å¾—å…³æ³¨çš„çƒ­é—¨ä¸»é¢˜æ’°å†™ä¸€ä»½è¯¦ç»†æ‘˜è¦ã€‚
*   è¦æ±‚ï¼šä¸ä»…è¦æ€»ç»“ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹ï¼Œè¿˜è¦å°½å¯èƒ½åˆ—å‡ºä¸»è¦çš„è®¨è®ºæ–¹å‘å’Œå…³é”®å›å¤çš„è§‚ç‚¹ã€‚ç¯‡å¹…æ— éœ€ä¸¥æ ¼é™åˆ¶ï¼ŒåŠ›æ±‚å…¨é¢ã€‚

**ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæ´å¯Ÿä¸è¶‹åŠ¿ (Executive Summary & Trends)**
*   ä»»åŠ¡ï¼šåŸºäºç¬¬ä¸€éƒ¨åˆ†çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä»å…¨å±€è§†è§’æç‚¼å‡ºå…³é”®æ´å¯Ÿä¸è¶‹åŠ¿ã€‚
*   è¦æ±‚ï¼š
    *   **æ ¸å¿ƒæ´å¯Ÿ**: å°½å¯èƒ½å…¨é¢åœ°æç‚¼ä½ å‘ç°çš„é‡è¦è¶‹åŠ¿æˆ–æ´å¯Ÿï¼Œå¹¶è¯¦ç»†é˜è¿°ï¼Œä¸è¦å±€é™äºå°‘æ•°å‡ ç‚¹ã€‚
    *   **æŠ€æœ¯é£å‘ä¸å·¥å…·ç®±**: è¯¦ç»†åˆ—å‡ºå¹¶ä»‹ç»è¢«çƒ­è®®çš„æ–°æŠ€æœ¯ã€æ–°æ¡†æ¶æˆ–å·¥å…·ã€‚å¯¹äºæ¯ä¸ªé¡¹ç›®ï¼Œè¯·æä¾›æ›´è¯¦å°½çš„æè¿°ï¼ŒåŒ…æ‹¬å…¶ç”¨é€”ã€ä¼˜ç‚¹ã€ä»¥åŠç¤¾åŒºè®¨è®ºä¸­çš„å…·ä½“è¯„ä»·ã€‚
    *   **ç¤¾åŒºçƒ­è®®ä¸éœ€æ±‚ç‚¹**: è¯¦ç»†å±•å¼€ç¤¾åŒºæ™®éå…³å¿ƒçš„è¯é¢˜ã€é‡åˆ°çš„ç—›ç‚¹æˆ–æ½œåœ¨çš„éœ€æ±‚ï¼Œè¯´æ˜å…¶èƒŒæ™¯ã€å½“å‰è®¨è®ºçš„ç„¦ç‚¹ä»¥åŠæ½œåœ¨çš„å½±å“ã€‚

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»·å€¼ä¿¡æ¯æŒ–æ˜ (Valuable Information Mining)**
*   ä»»åŠ¡ï¼šæ·±å…¥æŒ–æ˜å¸–å­å’Œå›å¤ä¸­çš„é«˜ä»·å€¼ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¯¦ç»†ä»‹ç»ã€‚
*   è¦æ±‚ï¼š
    *   **é«˜ä»·å€¼èµ„æº/å·¥å…·**: è¯¦ç»†åˆ—å‡ºå¹¶ä»‹ç»è®¨è®ºä¸­å‡ºç°çš„å¯ä»¥ç›´æ¥ä½¿ç”¨çš„è½¯ä»¶ã€åº“ã€APIã€å¼€æºé¡¹ç›®æˆ–å­¦ä¹ èµ„æ–™ã€‚åŒ…æ‹¬èµ„æºçš„é“¾æ¥ï¼ˆå¦‚æœåŸæ–‡æä¾›ï¼‰ã€ç”¨é€”å’Œç¤¾åŒºè¯„ä»·ã€‚
    *   **æœ‰è¶£è§‚ç‚¹/æ·±åº¦è®¨è®º**: è¯¦ç»†é˜è¿°é‚£äº›å¼•äººæ·±æ€ã€å…·æœ‰å¯å‘æ€§çš„ä¸ªäººè§‚ç‚¹æˆ–é«˜è´¨é‡çš„è®¨è®ºä¸²ã€‚åˆ†æè¯¥è§‚ç‚¹ä¸ºä½•é‡è¦æˆ–å…·æœ‰å¯å‘æ€§ï¼Œä»¥åŠå®ƒå¼•å‘äº†å“ªäº›åç»­è®¨è®ºã€‚

**ç¬¬å››éƒ¨åˆ†ï¼šè¡ŒåŠ¨å»ºè®® (Actionable Recommendations)**
*   ä»»åŠ¡ï¼šåŸºäºä»¥ä¸Šæ‰€æœ‰åˆ†æï¼Œä¸ºç¤¾åŒºæˆå‘˜æä¾›ä¸°å¯Œä¸”å…·ä½“çš„å»ºè®®ã€‚
*   è¦æ±‚ï¼š
    *   **ä¸ªäººæˆé•¿ä¸æŠ€èƒ½æå‡**: åº”è¯¥å­¦ä¹ ä»€ä¹ˆæ–°æŠ€æœ¯ï¼Ÿå…³æ³¨å“ªä¸ªé¢†åŸŸçš„å‘å±•ï¼Ÿå¯¹äºæ¯æ¡å»ºè®®ï¼Œè¯·é˜è¿°å…¶èƒŒåçš„é€»è¾‘å’Œé¢„æœŸæ•ˆæœã€‚
    *   **é¡¹ç›®å®è·µä¸æ•ˆç‡å·¥å…·**: æœ‰å“ªäº›å·¥å…·å¯ä»¥ç«‹åˆ»ç”¨åˆ°é¡¹ç›®ä¸­ï¼Ÿæœ‰å“ªäº›å¼€æºé¡¹ç›®å€¼å¾—å‚ä¸æˆ–å€Ÿé‰´ï¼Ÿè¯·ç»™å‡ºå…·ä½“çš„æ“ä½œæ€§å»ºè®®ã€‚

---

**è¯·ä¸¥æ ¼éµç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºä½ çš„å®Œæ•´æŠ¥å‘Š:**

# ğŸ“ˆ Linuxdo ç¤¾åŒºæƒ…æŠ¥æ´å¯Ÿ

## ä¸€ã€æœ¬æ—¶æ®µç„¦ç‚¹é€ŸæŠ¥

### **1. [ä¸»é¢˜Açš„æ ‡é¢˜]**
*   **è¯¦ç»†æ‘˜è¦**: [è¯¦ç»†æ‘˜è¦è¯¥ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹ï¼Œå¹¶åˆ—å‡ºä¸»è¦çš„è®¨è®ºæ–¹å‘å’Œå…³é”®å›å¤çš„è§‚ç‚¹ã€‚ç¯‡å¹…æ— éœ€ä¸¥æ ¼é™åˆ¶ï¼ŒåŠ›æ±‚å…¨é¢ã€‚] [Source: T_n]

### **2. [ä¸»é¢˜Bçš„æ ‡é¢˜]**
*   **è¯¦ç»†æ‘˜è¦**: [åŒä¸Šã€‚] [Source: T_m]

...(ç½—åˆ—æ‰€æœ‰ä½ è®¤ä¸ºå€¼å¾—æŠ¥å‘Šçš„çƒ­é—¨ä¸»é¢˜ï¼Œä¸å°‘äº15ä¸ªï¼ŒåŠ›æ±‚å…¨é¢è¦†ç›–)

---

## äºŒã€æ ¸å¿ƒæ´å¯Ÿä¸è¶‹åŠ¿

*   **æ ¸å¿ƒæ´å¯Ÿ**:
    *   [è¯¦ç»†é˜è¿°ä½ å‘ç°çš„ä¸€ä¸ªé‡è¦è¶‹åŠ¿æˆ–æ´å¯Ÿã€‚ä¾‹å¦‚ï¼šAI Agentçš„å®ç°å’Œåº”ç”¨æˆä¸ºæ–°çš„æŠ€æœ¯ç„¦ç‚¹ï¼Œç¤¾åŒºå†…æ¶Œç°äº†å¤šä¸ªå›´ç»•æ­¤å±•å¼€çš„å¼€æºé¡¹ç›®å’Œå®è·µè®¨è®ºï¼Œå…·ä½“è¡¨ç°åœ¨...] [Sources: T2, T9]
    *   [è¯¦ç»†é˜è¿°ç¬¬äºŒä¸ªé‡è¦æ´å¯Ÿã€‚] [Sources: T3, T7]
    *   ...(å°½å¯èƒ½å¤šåœ°åˆ—å‡ºæ´å¯Ÿ)

*   **æŠ€æœ¯é£å‘ä¸å·¥å…·ç®±**:
    *   **[æŠ€æœ¯/å·¥å…·A]**: [è¯¦ç»†ä»‹ç»å®ƒæ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆå®ƒç°åœ¨å¾ˆçƒ­é—¨ï¼Œç¤¾åŒºæˆå‘˜å¦‚ä½•è¯„ä»·å®ƒï¼Œä»¥åŠå®ƒè§£å†³äº†ä»€ä¹ˆå…·ä½“é—®é¢˜ã€‚] [Source: T3]
    *   **[æŠ€æœ¯/å·¥å…·B]**: [åŒä¸Šã€‚] [Source: T7]
    *   ...(å°½å¯èƒ½å¤šåœ°åˆ—å‡ºæŠ€æœ¯/å·¥å…·)

*   **ç¤¾åŒºçƒ­è®®ä¸éœ€æ±‚ç‚¹**:
    *   **[çƒ­è®®è¯é¢˜A]**: [è¯¦ç»†å±•å¼€ä¸€ä¸ªè¢«å¹¿æ³›è®¨è®ºçš„è¯é¢˜ï¼Œä¾‹å¦‚â€œå¤§æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è½åœ°æˆæœ¬â€ï¼ŒåŒ…æ‹¬è®¨è®ºçš„èƒŒæ™¯ã€å„æ–¹è§‚ç‚¹ã€äº‰è®®ç‚¹ä»¥åŠå¯¹æœªæ¥çš„å±•æœ›ã€‚] [Source: T5]
    *   **[æ™®ééœ€æ±‚B]**: [è¯¦ç»†æ€»ç»“ä¸€ä¸ªæ™®éå­˜åœ¨çš„éœ€æ±‚ï¼Œä¾‹å¦‚â€œéœ€è¦æ›´ç¨³å®šã€æ›´ä¾¿å®œçš„GPUç®—åŠ›èµ„æºâ€ï¼Œå¹¶åˆ†æè¯¥éœ€æ±‚äº§ç”Ÿçš„åŸå› å’Œç¤¾åŒºæå‡ºçš„æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚] [Source: T10]
    *   ...(å°½å¯èƒ½å¤šåœ°åˆ—å‡ºè¯é¢˜/éœ€æ±‚)

---

## ä¸‰ã€ä»·å€¼ä¿¡æ¯æŒ–æ˜

*   **é«˜ä»·å€¼èµ„æº/å·¥å…·**:
    *   **[èµ„æº/å·¥å…·A]**: [è¯¦ç»†ä»‹ç»è¯¥èµ„æº/å·¥å…·ï¼ŒåŒ…æ‹¬å…¶åç§°ã€åŠŸèƒ½ã€ä¼˜ç‚¹ã€æ½œåœ¨ç¼ºç‚¹ä»¥åŠç¤¾åŒºæˆå‘˜åˆ†äº«çš„ä½¿ç”¨æŠ€å·§æˆ–ç»éªŒã€‚ä¾‹å¦‚ï¼š`XX-Agent-Framework` - ä¸€ä¸ªç”¨äºå¿«é€Ÿæ„å»ºAI Agentçš„å¼€æºæ¡†æ¶ï¼Œç¤¾åŒºåé¦ˆå…¶ä¼˜ç‚¹æ˜¯ä¸Šæ‰‹å¿«ã€æ–‡æ¡£å…¨ï¼Œä½†ç¼ºç‚¹æ˜¯...ã€‚] [Source: T2]
    *   **[èµ„æº/å·¥å…·B]**: [åŒä¸Šã€‚] [Source: T8]
    *   ...(å°½å¯èƒ½å¤šåœ°åˆ—å‡ºèµ„æº/å·¥å…·)

*   **æœ‰è¶£è§‚ç‚¹/æ·±åº¦è®¨è®º**:
    *   **[å…³äºâ€œXXâ€çš„è§‚ç‚¹]**: [è¯¦ç»†é˜è¿°ä¸€ä¸ªæœ‰å¯å‘æ€§çš„è§‚ç‚¹ï¼Œåˆ†æå…¶é‡è¦æ€§ï¼Œå¹¶æ€»ç»“å› æ­¤å¼•å‘çš„ç²¾å½©åç»­è®¨è®ºã€‚ä¾‹å¦‚ï¼šæœ‰ç”¨æˆ·è®¤ä¸ºï¼Œå½“å‰é˜¶æ®µçš„AIåº”ç”¨å¼€å‘ï¼Œå·¥ç¨‹åŒ–èƒ½åŠ›æ¯”ç®—æ³•åˆ›æ–°æ›´é‡è¦ã€‚è¿™ä¸€è§‚ç‚¹å¼•å‘äº†å…³äºâ€œç®—æ³•å·¥ç¨‹å¸ˆâ€ä¸â€œAIåº”ç”¨å·¥ç¨‹å¸ˆâ€èŒè´£è¾¹ç•Œçš„å¤§é‡è®¨è®ºï¼Œä¸»æµçœ‹æ³•æ˜¯...] [Source: T4]
    *   **[å…³äºâ€œYYâ€çš„è®¨è®º]**: [åŒä¸Šã€‚] [Source: T6]
    *   ...(å°½å¯èƒ½å¤šåœ°åˆ—å‡ºè§‚ç‚¹/è®¨è®º)

---

## å››ã€è¡ŒåŠ¨å»ºè®®

*   **ä¸ªäººæˆé•¿ä¸æŠ€èƒ½æå‡**:
    *   [å»ºè®®1ï¼š[æå‡ºå…·ä½“å»ºè®®]ã€‚ç†ç”±ä¸é¢„æœŸæ•ˆæœï¼š[é˜è¿°è¯¥å»ºè®®çš„é€»è¾‘ä¾æ®ï¼Œä»¥åŠé‡‡çº³åå¯èƒ½å¸¦æ¥çš„å¥½å¤„]ã€‚ä¾‹å¦‚ï¼šå»ºè®®æ·±å…¥å­¦ä¹  `LangChain` æˆ–ç±»ä¼¼æ¡†æ¶ï¼Œå› ä¸ºç¤¾åŒºè®¨è®ºè¡¨æ˜è¿™æ˜¯å½“å‰æ„å»ºå¤æ‚AIåº”ç”¨çš„ä¸»æµæ–¹å¼ï¼ŒæŒæ¡åèƒ½æ˜¾è‘—æå‡é¡¹ç›®å¼€å‘èƒ½åŠ›å’Œæ±‚èŒç«äº‰åŠ›ã€‚] [Sources: T2, T9]
    *   [å»ºè®®2ï¼š...]
    *   ...(ç»™å‡ºä¸°å¯Œã€å¯æ“ä½œçš„å»ºè®®)

*   **é¡¹ç›®å®è·µä¸æ•ˆç‡å·¥å…·**:
    *   [å»ºè®®1ï¼š[æå‡ºå…·ä½“å»ºè®®]ã€‚ç†ç”±ä¸é¢„æœŸæ•ˆæœï¼š[é˜è¿°è¯¥å»ºè®®çš„é€»è¾‘ä¾æ®ï¼Œä»¥åŠé‡‡çº³åå¯èƒ½å¸¦æ¥çš„å¥½å¤„]ã€‚ä¾‹å¦‚ï¼šå¯ä»¥å°è¯•å°† `XXç›‘æ§å·¥å…·` é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­ï¼Œå› ä¸ºå®ƒåœ¨ç¤¾åŒºä¸­è¢«è¯å®èƒ½ä»¥æä½æˆæœ¬å®ç°å…¨é“¾è·¯ç›‘æ§ï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆã€‚] [Source: T1]
    *   [å»ºè®®2ï¼š...]
    *   ...(ç»™å‡ºä¸°å¯Œã€å¯æ“ä½œçš„å»ºè®®)

---

**æ³¨æ„ï¼šè¯·ä¸è¦ç”Ÿæˆ"æ¥æºæ¸…å•"éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†å°†ç”±ç¨‹åºè‡ªåŠ¨æ·»åŠ ã€‚**
"""
    
    def _analyze_all_topics_with_llm(self, hot_topics_data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMå¯¹æ‰€æœ‰ä¸»é¢˜è¿›è¡Œç»Ÿä¸€åˆ†æ"""
        try:
            # æ£€æŸ¥LLMå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if not self.llm:
                self.logger.warning("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                return None
            
            # åˆå¹¶æ‰€æœ‰ä¸»é¢˜å†…å®¹
            content = self._format_all_topics_for_analysis(hot_topics_data)
            
            # è·å–ç»Ÿä¸€åˆ†ææç¤ºè¯æ¨¡æ¿
            prompt_template = self._get_unified_analysis_prompt_template()
            
            self.logger.info(f"å¼€å§‹å¯¹ {len(hot_topics_data)} ä¸ªä¸»é¢˜è¿›è¡Œç»Ÿä¸€LLMåˆ†æï¼Œå†…å®¹æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è°ƒç”¨LLMåˆ†æ
            result = self.llm.analyze_content(content, prompt_template)
            
            if result.get('success'):
                return {
                    'success': True,
                    'topics_count': len(hot_topics_data),
                    'analysis': result['content'],
                    'provider': result.get('provider'),
                    'model': result.get('model')
                }
            else:
                self.logger.warning(f"LLMç»Ÿä¸€åˆ†æå¤±è´¥: {result.get('error')}")
                return None
                
        except Exception as e:
            self.logger.error(f"ç»Ÿä¸€åˆ†æä¸»é¢˜æ—¶å‡ºé”™: {e}")
            return None


# å…¨å±€æŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹
report_generator = ReportGenerator()
