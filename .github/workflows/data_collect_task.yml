name: Linux.do info collector

on:
  schedule:
    # 每30分钟运行一次爬取任务，北京时间早7点到晚24点（UTC时间23:00-15:59）
    - cron: '0,30 23 * * *'
    - cron: '0,30 0-15 * * *'
    # 每3小时运行一次智能分析报告任务，北京时间早8点到晚22点（UTC时间0-14点，2小时间隔）
    - cron: '0 0-14/3 * * *'
    # 每2天凌晨2点运行清理任务，北京时间凌晨2点（UTC前一天18点）
    - cron: '0 18 */2 * *'
  workflow_dispatch:
    inputs:
      task:
        description: '任务类型'
        required: true
        default: 'crawl'
        type: choice
        options:
        - crawl
        - cleanup
        - stats
        - analysis
        - report
        - full
      retention_days:
        description: '数据保留天数（仅用于cleanup任务）'
        required: false
        default: '120'

jobs:
  crawl:
    runs-on: ubuntu-latest
    if: (github.event_name == 'schedule' && contains('0,30 23 * * *;0,30 0-15 * * *', github.event.schedule)) || (github.event_name == 'workflow_dispatch' && github.event.inputs.task == 'crawl')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Install crawler dependencies
      run: |
        pip install -r requirements-crawler.txt
        
    - name: Install Playwright browsers
      run: playwright install --with-deps chromium
        
    - name: Run crawler
      run: python main.py --task crawl --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        TARGETS: ${{ secrets.TARGETS }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}
      
  cleanup:
    runs-on: ubuntu-latest
    if: (github.event_name == 'schedule' && github.event.schedule == '0 18 */2 * *') || (github.event_name == 'workflow_dispatch' && github.event.inputs.task == 'cleanup')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies only
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Run cleanup
      run: python main.py --task cleanup --retention-days ${{ github.event.inputs.retention_days || 180 }} --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        TARGETS: ${{ secrets.TARGETS }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}
      
  stats:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.task == 'stats'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies only
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Get stats
      run: python main.py --task stats --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        TARGETS: ${{ secrets.TARGETS }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}
      
  analysis:
    runs-on: ubuntu-latest
    needs: crawl
    if: github.event_name == 'schedule' && contains('0,30 23 * * *;0,30 0-15 * * *', github.event.schedule)
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies only
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Run hotness analysis
      run: python main.py --task analysis --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        TARGETS: ${{ secrets.TARGETS }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}
      
  manual-analysis:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.task == 'analysis'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies only
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Run hotness analysis
      run: python main.py --task analysis --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        TARGETS: ${{ secrets.TARGETS }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}
      
  full:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.task == 'full'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Install crawler dependencies
      run: |
        pip install -r requirements-crawler.txt
        
    - name: Install Playwright browsers
      run: playwright install --with-deps chromium
        
    - name: Run full maintenance
      run: python main.py --task full --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        TARGETS: ${{ secrets.TARGETS }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}

  report:
    runs-on: ubuntu-latest
    if: (github.event_name == 'schedule' && github.event.schedule == '0 0-14/3 * * *') || (github.event_name == 'workflow_dispatch' && github.event.inputs.task == 'report')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install base dependencies only
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-base.txt
        
    - name: Run intelligent analysis report
      run: python main.py --task report --output json
      env:
        # --- Required Secrets ---
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        TARGETS: ${{ secrets.TARGETS }}

        # --- LLM API Keys (at least one required for report generation) ---
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        
        # --- Optional LLM Configuration ---
        OPENAI_MODELS: ${{ secrets.OPENAI_MODELS }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        DEEPSEEK_MODEL: ${{ secrets.DEEPSEEK_MODEL }}
        DEEPSEEK_BASE_URL: ${{ secrets.DEEPSEEK_BASE_URL }}
        LLM_PREFERRED_PROVIDER: ${{ secrets.LLM_PREFERRED_PROVIDER }}
        LLM_MAX_RETRIES: ${{ secrets.LLM_MAX_RETRIES }}
        LLM_TIMEOUT: ${{ secrets.LLM_TIMEOUT }}
        LLM_MAX_CONTENT_LENGTH: ${{ secrets.LLM_MAX_CONTENT_LENGTH }}
        REPORT_TOP_TOPICS: ${{ secrets.REPORT_TOP_TOPICS }}

        # --- Notion Integration (optional) ---
        NOTION_INTEGRATION_TOKEN: ${{ secrets.NOTION_INTEGRATION_TOKEN }}
        NOTION_PARENT_PAGE_ID: ${{ secrets.NOTION_PARENT_PAGE_ID }}

        # --- Optional Secrets (with defaults in code) ---
        CRAWLER_SCAN_PAGES: ${{ secrets.CRAWLER_SCAN_PAGES }}
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_CONCURRENT_BOARDS: ${{ secrets.CRAWLER_MAX_CONCURRENT_BOARDS }}
        CRAWLER_MAX_CONCURRENT_PAGES: ${{ secrets.CRAWLER_MAX_CONCURRENT_PAGES }}
        CRAWLER_MAX_CONCURRENT_DETAILS: ${{ secrets.CRAWLER_MAX_CONCURRENT_DETAILS }}
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        LOGGING_LOG_LEVEL: INFO
        LOGGING_LOG_FILE: ${{ secrets.LOGGING_LOG_FILE }}
